from fastapi import APIRouter, Depends, HTTPException, status, Query
from sqlalchemy.orm import Session, joinedload
from typing import List, Dict, Optional
from pathlib import Path as FilePath
from datetime import datetime, timezone

from app.database import get_db
from app import models, schemas
from app.schemas import ArtifactType

##########################################################################
#
# Globals
#
##########################################################################

router = APIRouter()

DATA_ROOT = FilePath("/data/ML/big-data-full")

# Map filename patterns to artifact_type strings
# NOTE: visit https://github.com/aryrfjr/PRM_4_113603/tree/main/scripts for
#   more information on the .sh scripts mentioned below.
EXPECTED_RUN_ARTIFACTS = {  # NOTE: use template.format(NC=nominal_composition)
    "zca-th300.dump": ArtifactType.LAMMPS_DUMP,  # generated by zca-bd-full-SD-cpu.sh; Run
    "log.lammps": ArtifactType.LAMMPS_LOG,  # generated by zca-bd-full-SD-cpu.sh; Run
    "{NC}.lmp.inp": ArtifactType.LAMMPS_INPUT,  # generated by zca-bd-full-SD-cpu.sh; Run
    "{NC}.lmp.out": ArtifactType.LAMMPS_OUTPUT,  # generated by zca-bd-full-SD-cpu.sh; Run
}

EXPECTED_SUB_RUN_ARTIFACTS = {  # NOTE: use template.format(NC=nominal_composition)
    "{NC}.scf.in": ArtifactType.QE_SCF_IN,  # generated by setup_ICOHP_jobs.py
    "{NC}.xyz": ArtifactType.LAMMPS_DUMP_XYZ,  # generated by setup_ICOHP_jobs.py
    "lobsterin": ArtifactType.LOBSTER_INPUT,  # generated by setup_ICOHP_jobs.py
    "lobsterin-quippy": ArtifactType.LOBSTER_INPUT_BND,  # generated by setup_ICOHP_jobs.py
    "SOAPS.vec": ArtifactType.SOAP_VECTORS,  # generated by setup_ICOHP_jobs.py
    "{NC}.scf.out": ArtifactType.QE_SCF_OUT,  # generated by zca-QE-SD_cpu.sh
    "lobsterout": ArtifactType.LOBSTER_RUN_OUTPUT,  # generated by zca-LOB-SD_cpu.sh
    "{NC}.lb.out": ArtifactType.LOBSTER_OUTPUT,  # generated by zca-LOB-SD_cpu.sh
    "ICOHPLIST.lobster": ArtifactType.ICOHPLIST,  # generated by zca-LOB-SD_cpu.sh
}

##########################################################################
#
# Helpers
#
##########################################################################


#
# Register known artifact files in sub_run_path and registers them in the DB.
#
##########################################################################
def register_artifacts(
    nominal_composition: str,
    path: FilePath,
    expected_artifacts: Dict,
) -> List[
    models.SimulationArtifact
]:  # each element in the returned List is an SQLAlchemy ORM object

    artifacts = []

    for filename, artifact_type in expected_artifacts.items():

        file_path = path / filename.format(NC=nominal_composition)

        if file_path.exists():

            artifact = models.SimulationArtifact(
                artifact_type=artifact_type.value,
                file_path=str(file_path),
                file_size=file_path.stat().st_size,
                checksum=None,  # TODO: compute SHA256 here
                created_at=datetime.now(timezone.utc),
            )

            artifacts.append(artifact)

    return artifacts


#
# Detects known artifact files in sub_run_path and registers them in the DB.
#
##########################################################################
def detect_and_register_artifacts(
    nominal_composition: str,
    sub_run_number: int,
    sub_run_path: FilePath,
    run_path: Optional[FilePath],
) -> List[
    models.SimulationArtifact
]:  # each element in the returned List is an SQLAlchemy ORM object

    artifacts = []

    # NOTE: I was going to define a new table to separate artifacts related
    #   to Runs and SubRuns, but I decided to simply make SubRun 0 to have
    #   additional artifacts related to the LAMMPS simulations, since that
    #   SubRun is the reference structure.
    if sub_run_number == 0:

        if run_path is None:
            raise ValueError("run_path must be provided when sub_run_number is 0")

        artifacts.extend(
            register_artifacts(nominal_composition, run_path, EXPECTED_RUN_ARTIFACTS)
        )

    artifacts.extend(
        register_artifacts(
            nominal_composition, sub_run_path, EXPECTED_SUB_RUN_ARTIFACTS
        )
    )

    return artifacts


#
# Endpoints scoped to the Data Generation & Labeling (DataOps) phase,
# which includes the following steps:
#
# - Generate (DataOps phase; exploration/exploitation)
# - ETL model (DataOps phase; Feature Store Lite)
#
########################################################################


# Schedules configuration space exploration for a given nominal composition
@router.post(
    "/generate/{nominal_composition}",
    response_model=schemas.GenericStatusResponse,
    status_code=status.HTTP_202_ACCEPTED,
    tags=["DataOps"],
)
def schedule_exploration(
    nominal_composition: str,
    payload: schemas.ScheduleExplorationRequest,
    db: Session = Depends(get_db),
):

    for i in range(payload.num_simulations):

        #
        # Check that the NominalComposition exists (TODO: replicated R1)
        #
        ########################################################################
        nc = (
            db.query(models.NominalComposition)
            .filter_by(name=nominal_composition)
            .first()
        )

        if not nc:
            raise HTTPException(
                status_code=404,
                detail=f"NominalComposition '{nominal_composition}' not found",
            )

        #
        # Checking the consistency of directories (TODO: replicated R2)
        #
        ########################################################################

        last_run = (
            db.query(models.Run)
            .filter(models.Run.nominal_composition_id == nc.id)
            .order_by(models.Run.run_number.desc())
            .first()
        )

        next_run_number = (last_run.run_number if last_run else 0) + 1

        # TODO: handle gaps in the sequence of ID_RUN directories
        nc_dir = DATA_ROOT / nc.name
        nc_id_run_dir = nc_dir / "c/md/lammps/100" / str(next_run_number)
        nc_sub_run_dir = nc_id_run_dir / "2000/0"

        if not nc_dir.exists():
            raise HTTPException(
                status_code=404,
                detail=f"Directory for Nominal Composition '{nc.name}' not found",
            )

        if not nc_id_run_dir.exists() or not nc_sub_run_dir.exists():
            raise HTTPException(
                status_code=404,
                detail=f"Directory for ID_RUN '{next_run_number}' or for SUB_RUN '0' not found for Nominal Composition '{nc.name}'",
            )

        #
        # Persisting to the DB (TODO: replicated R3)
        #
        ########################################################################

        # Create the SimulationArtifacts of default SubRun with id 0
        artifacts = detect_and_register_artifacts(
            nc.name, 0, nc_sub_run_dir, nc_id_run_dir
        )

        # Create the default SubRun with id 0
        sub_run = models.SubRun(
            sub_run_number=0,
            simulation_artifacts=artifacts,
            status=schemas.Status.SCHEDULED.value,  # optional: default already; TODO: build an application that will change it
        )

        # Create the Run and attach the SubRun
        run = models.Run(
            nominal_composition_id=nc.id,
            run_number=next_run_number,
            status=schemas.Status.SCHEDULED.value,  # optional: default already; TODO: build an application that will change it
            sub_runs=[sub_run],  # ORM relationship binds them
        )

        # Persist both Run and SubRun with its SimulationArtifacts
        db.add(run)
        db.commit()

    # TODO: if one of the runs fail for some reason must notify the user
    return schemas.GenericStatusResponse(
        message=f"Exploration scheduled for '{nominal_composition}'.",
        status=schemas.Status.SCHEDULED.value,
    )


# Schedules geometry augmentation (exploitation) for a given nominal composition and runs
@router.post(
    "/generate/{nominal_composition}/augment",
    response_model=schemas.GenericStatusResponse,
    status_code=status.HTTP_202_ACCEPTED,
    tags=["DataOps"],
)
def schedule_augmentation(
    nominal_composition: str,
    payload: schemas.ScheduleExploitationRequest,
    db: Session = Depends(get_db),
):

    #
    # Check that the NominalComposition exists (TODO: replicated R1)
    #
    ########################################################################

    nc = db.query(models.NominalComposition).filter_by(name=nominal_composition).first()

    if not nc:
        raise HTTPException(
            status_code=404,
            detail=f"NominalComposition '{nominal_composition}' not found",
        )

    #
    # Validate the input payload against the database. Checking if
    # all Runs exist and that all corresponding SubRuns are new records.
    #
    ########################################################################

    # Check Run IDs
    run_ids = [item.id for item in payload.runs]
    existing_runs = (
        db.query(models.Run)
        .filter(models.Run.id.in_(run_ids))
        .options(joinedload(models.Run.sub_runs))
        .all()
    )

    existing_run_ids = {run.id for run in existing_runs}
    missing_run_ids = set(run_ids) - existing_run_ids

    if missing_run_ids:
        raise HTTPException(
            status_code=404,
            detail=f"The following run IDs were not found for NominalComposition '{nominal_composition}': {missing_run_ids}",
        )

    # Check SubRun IDs
    requested_sub_run_map = {item.id: set(item.sub_runs) for item in payload.runs}

    for run in existing_runs:

        existing_sub_run_numbers = {sr.sub_run_number for sr in run.sub_runs}
        submitted_sub_run_numbers = requested_sub_run_map.get(run.id, set())

        overlap = [
            sub_run_number
            for sub_run_number in submitted_sub_run_numbers
            if sub_run_number in existing_sub_run_numbers
        ]

        if overlap:
            raise HTTPException(
                status_code=404,
                detail=f"The following sub_run IDs already exist for NominalComposition '{nominal_composition}' and run_number '{run.run_number}': {overlap}.",
            )

    #
    # Checking the consistency of directories (TODO: replicated R2)
    #
    ########################################################################

    nc_dir = DATA_ROOT / nominal_composition

    if not nc_dir.exists():
        raise HTTPException(
            status_code=404,
            detail=f"Directory for Nominal Composition '{nominal_composition}' not found",
        )

    for run in existing_runs:

        nc_id_run_dir = nc_dir / "c/md/lammps/100" / str(run.run_number)
        if not nc_id_run_dir.exists():
            raise HTTPException(
                status_code=404,
                detail=f"Directory for ID_RUN '{run.run_number}' not found for Nominal Composition '{nominal_composition}'",
            )

        submitted_sub_run_numbers = requested_sub_run_map.get(run.id, set())

        for sub_run_number in submitted_sub_run_numbers:

            nc_sub_run_dir = nc_id_run_dir / "2000/" / str(sub_run_number)

            if not nc_sub_run_dir.exists():
                raise HTTPException(
                    status_code=404,
                    detail=f"Directory for SUB_RUN '{sub_run_number}' not found for Nominal Composition '{nominal_composition}' and run_number '{run.run_number}'.",
                )

    #
    # Persisting to the DB (TODO: replicated R3)
    #
    ########################################################################

    for run in existing_runs:

        submitted_sub_run_numbers = requested_sub_run_map.get(run.id, set())

        for sub_run_number in submitted_sub_run_numbers:

            nc_id_run_dir = nc_dir / "c/md/lammps/100" / str(run.run_number)
            nc_sub_run_dir = nc_id_run_dir / "2000/" / str(sub_run_number)

            # Create the SimulationArtifacts of SubRun
            # TODO: the last argument nc_id_run_dir should be optional but it is not working
            artifacts = detect_and_register_artifacts(
                nominal_composition, sub_run_number, nc_sub_run_dir, nc_id_run_dir
            )

            # Create the SubRun
            sub_run = models.SubRun(
                sub_run_number=sub_run_number,
                simulation_artifacts=artifacts,
                status=schemas.Status.SCHEDULED.value,  # optional: default already; TODO: build an application that will change it
            )

            # Attach the SubRun to the run
            run.sub_runs.append(sub_run)

        # Persist SubRuns with their SimulationArtifacts
        db.commit()

    return schemas.GenericStatusResponse(
        message=f"Augmentation scheduled for '{nominal_composition}'.",
        status=schemas.Status.SCHEDULED.value,
    )


# List All exploration jobs (Runs) created for a given Nominal Composition
@router.get(
    "/exploration_jobs/{nominal_composition}",
    tags=["DataOps"],
)
def list_nominal_composition_runs(
    nominal_composition: str,
    exploitation_info: bool = Query(
        default=False,
        description="Whether to include exploitation (augmentation; SubRuns) details",
    ),
    db: Session = Depends(get_db),
):

    query = (
        db.query(models.Run)
        .join(models.Run.nominal_composition)
        .filter(models.NominalComposition.name == nominal_composition)
    )

    if exploitation_info:
        query = query.order_by(models.Run.run_number).options(
            joinedload(models.Run.sub_runs)
        )

    runs = query.all()

    if exploitation_info:
        return [schemas.ExplorationJobFullResponse.from_orm(run) for run in runs]
    else:
        return [schemas.ExplorationJobBaseResponse.from_orm(run) for run in runs]


# Schedules ETL model (DBI building) for a given nominal composition
@router.post(
    "/etl_model",
    response_model=schemas.ETLModelResponse,
    status_code=status.HTTP_202_ACCEPTED,
    tags=["DataOps"],
)
def schedule_etl_model(payload: schemas.ETLModelRequest, db: Session = Depends(get_db)):

    # NOTE: The ETL model is a two step process originally implemented with the
    # scripts 'create_SSDB.py' (for a single NC) and 'mix_SSDBs.py' (for multiple NCs).
    #
    # TODO: update the request payload to meet that reality.

    return schemas.ETLModelResponse(
        message=f"ETL model build scheduled for '{payload.nominal_composition}'.",
        status=schemas.Status.SCHEDULED.value,
    )
